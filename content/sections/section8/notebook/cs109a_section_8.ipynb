{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Standard Section 8: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost.\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Hayden Joy<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will work with a spam email dataset again. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will review Decision Trees, Bagging, and Random Forest methods, and introduce Boosting: Ada Boost and XGBoost.\n",
    "\n",
    "Specifically, we will: \n",
    "  \n",
    "1. *Quick review of last week*  \n",
    "2. Trees in the context of the bias‚Äîvariance tradeoff\n",
    "3. Rebuild the Decision Tree model, Bagging model, Random Forest Model just for comparison with Boosting. \n",
    "4. *Theory:* What is Boosting?\n",
    "5. Use the Adaboost on the Spam Dataset.\n",
    "6. *Theory:* What is Gradient Boosting and XGBoost?\n",
    "7. Use XGBoost on the Spam Dataset: Extreme Gradient Boosting\n",
    "\n",
    "Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 1. *Quick review of last week*  \n",
    "\n",
    "### ensemble: a group of items viewed as a whole rather than individually\n",
    "\n",
    "#### The Idea: Decision Trees are just flowcharts and interpretable!\n",
    "\n",
    "It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire;\n",
    " - interpretable by humans \n",
    " - have sufficiently complex decision boundaries \n",
    " - the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "#### How to build Decision Trees (the Learning Algorithm in words): \n",
    "To learn a decision tree model, we take a greedy approach: \n",
    " 1. Start with an empty decision tree (undivided feature space) \n",
    " 2. Choose the ‚Äòoptimal‚Äô predictor on which to split and choose the ‚Äòoptimal‚Äô threshold value for splitting by applying a **splitting criterion (1)**\n",
    " 3. Recurse on on each new node until **stopping condition (2)** is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we need a (1) splitting criterion and a (2) stopping condition:\n",
    "\n",
    "  #### (1) Splitting criterion \n",
    "\n",
    "<img src=\"data/split2_adj.png\" alt=\"split2\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Stopping condition\n",
    "\n",
    "**Not stopping while building a deeper and deeper tree = 100% training accuracy; Yet we will overfit! \n",
    "\n",
    "To prevent the **overfitting** from happening, we should have stopping condition.\n",
    "\n",
    "-------------\n",
    "\n",
    "#### How do we go from Classification to Regression?\n",
    "\n",
    "- For classification, we return the majority class in the points of each leaf node. \n",
    "- For regression we return the average of the outputs for the points in each leaf node. \n",
    "\n",
    "-------------\n",
    "\n",
    "#### What is bagging?\n",
    "  \n",
    "One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. \n",
    "\n",
    " 1. **Bootstrap:** we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. \n",
    " 2. **AGgregatiING** for a given input, we output the averaged outputs of all the models for that input. \n",
    " \n",
    "This method is called **Bagging: B** ootstrap + **AGG**regat**ING**. \n",
    "\n",
    "-------------\n",
    "\n",
    "#### What is Random Forest? \n",
    "\n",
    "- **Many trees** make a **forest**.\n",
    "- **Many random trees** make a **random forest**.\n",
    "\n",
    "\n",
    "Random Forest is a modified form of bagging that creates ensembles of independent decision trees. \n",
    "To *de-correlate the trees*, we: \n",
    "1. train each tree on a separate bootstrap **random sample** of the full training set (same as in bagging) \n",
    "2. for each tree, at each split, we **randomly select a set of ùêΩ‚Ä≤ predictors from the full set of predictors.** (not done in bagging)\n",
    "3. From amongst the ùêΩ‚Ä≤  predictors, we select the optimal predictor and the optimal corresponding threshold for the split. \n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "### Let's talk about decision trees, bagging, and random forest in the context of bias and variance.\n",
    "<img src=\"bias_variance.png\" alt=\"split2\" width=\"40%\"/>\n",
    "<img src=\"fitting.png\" alt=\"split2\" width=\"40%\"/>\n",
    "\n",
    "#### When is a decision tree underfit? When is a decision tree overfit? Let's think about this in the concept of tree depth.\n",
    "\n",
    "#### Bagging enjoys the benefits of \n",
    "- High expressiveness (by using larger trees it is able to approximate complex functions and decision boundaries).\n",
    "- Low _ _ _  by averaging the prediction of all the models thus reducing the _ _ _  in the final prediction.\n",
    "\n",
    "#### What is the weakness of bagging?\n",
    "- In practice, the ensemble of trees tend to be **highly ___**\n",
    "- When could my bagging model be underfit? In what way does this apply to other ensemble methods?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## 2. Just re-building the tree models of last week\n",
    "\n",
    "### Rebuild the Decision Tree model, Bagging model and Random Forest Model for comparison with Boosting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column_1</th>\n",
       "      <th>Column_2</th>\n",
       "      <th>Column_3</th>\n",
       "      <th>Column_4</th>\n",
       "      <th>Column_5</th>\n",
       "      <th>Column_6</th>\n",
       "      <th>Column_7</th>\n",
       "      <th>Column_8</th>\n",
       "      <th>Column_9</th>\n",
       "      <th>Column_10</th>\n",
       "      <th>Column_11</th>\n",
       "      <th>Column_12</th>\n",
       "      <th>Column_13</th>\n",
       "      <th>Column_14</th>\n",
       "      <th>Column_15</th>\n",
       "      <th>Column_16</th>\n",
       "      <th>Column_17</th>\n",
       "      <th>Column_18</th>\n",
       "      <th>Column_19</th>\n",
       "      <th>Column_20</th>\n",
       "      <th>Column_21</th>\n",
       "      <th>Column_22</th>\n",
       "      <th>Column_23</th>\n",
       "      <th>Column_24</th>\n",
       "      <th>Column_25</th>\n",
       "      <th>Column_26</th>\n",
       "      <th>Column_27</th>\n",
       "      <th>Column_28</th>\n",
       "      <th>Column_29</th>\n",
       "      <th>Column_30</th>\n",
       "      <th>Column_31</th>\n",
       "      <th>Column_32</th>\n",
       "      <th>Column_33</th>\n",
       "      <th>Column_34</th>\n",
       "      <th>Column_35</th>\n",
       "      <th>Column_36</th>\n",
       "      <th>Column_37</th>\n",
       "      <th>Column_38</th>\n",
       "      <th>Column_39</th>\n",
       "      <th>Column_40</th>\n",
       "      <th>Column_41</th>\n",
       "      <th>Column_42</th>\n",
       "      <th>Column_43</th>\n",
       "      <th>Column_44</th>\n",
       "      <th>Column_45</th>\n",
       "      <th>Column_46</th>\n",
       "      <th>Column_47</th>\n",
       "      <th>Column_48</th>\n",
       "      <th>Column_49</th>\n",
       "      <th>Column_50</th>\n",
       "      <th>Column_51</th>\n",
       "      <th>Column_52</th>\n",
       "      <th>Column_53</th>\n",
       "      <th>Column_54</th>\n",
       "      <th>Column_55</th>\n",
       "      <th>Column_56</th>\n",
       "      <th>Column_57</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column_1  Column_2  Column_3  Column_4  Column_5  Column_6  Column_7  Column_8  Column_9  Column_10  Column_11  Column_12  Column_13  Column_14  Column_15  Column_16  Column_17  Column_18  Column_19  Column_20  Column_21  Column_22  Column_23  Column_24  Column_25  Column_26  Column_27  Column_28  Column_29  Column_30  Column_31  Column_32  Column_33  Column_34  Column_35  Column_36  Column_37  Column_38  Column_39  Column_40  Column_41  Column_42  Column_43  Column_44  Column_45  Column_46  Column_47  Column_48  Column_49  Column_50  Column_51  Column_52  Column_53  Column_54  Column_55  Column_56  Column_57  Spam\n",
       "0      0.00      0.64      0.64       0.0      0.32      0.00      0.00      0.00      0.00       0.00       0.00       0.64       0.00       0.00       0.00       0.32       0.00       1.29       1.93       0.00       0.96        0.0       0.00       0.00        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0       0.00        0.0        0.0       0.00        0.0        0.0       0.00        0.0       0.00       0.00        0.0        0.0       0.00      0.000        0.0      0.778      0.000      0.000      3.756         61        278     1\n",
       "1      0.21      0.28      0.50       0.0      0.14      0.28      0.21      0.07      0.00       0.94       0.21       0.79       0.65       0.21       0.14       0.14       0.07       0.28       3.47       0.00       1.59        0.0       0.43       0.43        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0       0.07        0.0        0.0       0.00        0.0        0.0       0.00        0.0       0.00       0.00        0.0        0.0       0.00      0.132        0.0      0.372      0.180      0.048      5.114        101       1028     1\n",
       "2      0.06      0.00      0.71       0.0      1.23      0.19      0.19      0.12      0.64       0.25       0.38       0.45       0.12       0.00       1.75       0.06       0.06       1.03       1.36       0.32       0.51        0.0       1.16       0.06        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0       0.00        0.0        0.0       0.06        0.0        0.0       0.12        0.0       0.06       0.06        0.0        0.0       0.01      0.143        0.0      0.276      0.184      0.010      9.821        485       2259     1\n",
       "3      0.00      0.00      0.00       0.0      0.63      0.00      0.31      0.63      0.31       0.63       0.31       0.31       0.31       0.00       0.00       0.31       0.00       0.00       3.18       0.00       0.31        0.0       0.00       0.00        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0       0.00        0.0        0.0       0.00        0.0        0.0       0.00        0.0       0.00       0.00        0.0        0.0       0.00      0.137        0.0      0.137      0.000      0.000      3.537         40        191     1\n",
       "4      0.00      0.00      0.00       0.0      0.63      0.00      0.31      0.63      0.31       0.63       0.31       0.31       0.31       0.00       0.00       0.31       0.00       0.00       3.18       0.00       0.31        0.0       0.00       0.00        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0        0.0       0.00        0.0        0.0       0.00        0.0        0.0       0.00        0.0       0.00       0.00        0.0        0.0       0.00      0.135        0.0      0.135      0.000      0.000      3.537         40        191     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import Dataframe and Set Column Names\n",
    "spam_df = pd.read_csv('data/spam.csv', header=None)\n",
    "columns = [\"Column_\"+str(i+1) for i in range(spam_df.shape[1]-1)] + ['Spam']\n",
    "spam_df.columns = columns\n",
    "display(spam_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Set : (3262, 58)\n",
      "Shape of Testing Set : (1339, 58)\n"
     ]
    }
   ],
   "source": [
    "#Let us split the dataset into a 70-30 split by using the following:\n",
    "#Split data into train and test\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(spam_df)) < 0.7\n",
    "data_train = spam_df[msk]\n",
    "data_test = spam_df[~msk]\n",
    "\n",
    "#Split predictor and response columns\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "print(\"Shape of Training Set :\",data_train.shape)\n",
    "print(\"Shape of Testing Set :\" ,data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Spam in Training Set \t : 39.18%.\n",
      "Percentage of Spam in Testing Set \t : 39.96%.\n"
     ]
    }
   ],
   "source": [
    "#Check Percentage of Spam in Train and Test Set\n",
    "percentage_spam_training = 100*y_train.sum()/len(y_train)\n",
    "percentage_spam_testing  = 100*y_test.sum()/len(y_test)\n",
    "                                                  \n",
    "print(\"Percentage of Spam in Training Set \\t : {:0.2f}%.\".format(percentage_spam_training))\n",
    "print(\"Percentage of Spam in Testing Set \\t : {:0.2f}%.\".format(percentage_spam_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### Fitting an Optimal Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best depth for single decision trees of last week\n",
    "best_depth = 7\n",
    "print(\"The best depth was found to be:\", best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalaute the performance at the best depth\n",
    "model_tree = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model_tree.fit(x_train, y_train)\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_trees_training = accuracy_score(y_train, model_tree.predict(x_train))\n",
    "acc_trees_testing  = accuracy_score(y_test,  model_tree.predict(x_test))\n",
    "\n",
    "print(\"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%}\".format(acc_trees_training))\n",
    "print(\"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%}\".format(acc_trees_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------\n",
    "\n",
    "### Fitting 100 Single Decision Trees while Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 100 # we tried a variety of numbers here\n",
    "\n",
    "#Creating model\n",
    "np.random.seed(0)\n",
    "model = DecisionTreeClassifier(max_depth=best_depth+5)\n",
    "\n",
    "#Initializing variables\n",
    "predictions_train = np.zeros((data_train.shape[0], n_trees))\n",
    "predictions_test = np.zeros((data_test.shape[0], n_trees))\n",
    "\n",
    "#Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    temp = data_train.sample(frac=1, replace=True)\n",
    "    response_variable = temp['Spam']\n",
    "    temp = temp.drop(['Spam'], axis=1)\n",
    "    \n",
    "    model.fit(temp, response_variable)  \n",
    "    predictions_train[:,i] = model.predict(x_train)   \n",
    "    predictions_test[:,i] = model.predict(x_test)\n",
    "    \n",
    "#Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ensemble the prediction of each bagged decision tree model\n",
    "def get_prediction(df, count=-1):\n",
    "    count = df.shape[1] if count==-1 else count\n",
    "    temp = df.iloc[:,0:count]\n",
    "    return np.mean(temp, axis=1)>0.5\n",
    "\n",
    "#Check Accuracy of Spam Detection in Train and Test Set\n",
    "acc_bagging_training = 100*accuracy_score(y_train, get_prediction(predictions_train, count=-1))\n",
    "acc_bagging_testing  = 100*accuracy_score(y_test, get_prediction(predictions_test, count=-1))\n",
    "\n",
    "print(\"Bagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Random Forest Model\n",
    "#Training\n",
    "model = RandomForestClassifier(n_estimators=n_trees, max_depth=best_depth+5)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_random_forest_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_random_forest_testing = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Random Forest: Accuracy, Training Set : {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: Accuracy, Testing Set :  {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare the performance of our 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Training Set \\t: {:.2%}\".format(acc_trees_training))\n",
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "\n",
    "print(\"\\nBagging: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_bagging_training))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "\n",
    "print(\"\\nRandom Forest: \\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_random_forest_training))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.estimators_[0].tree_.feature))\n",
    "print(len(model.estimators_[0].tree_.threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout Room 1: Exploring RandomForestClassifier class instances.\n",
    "\n",
    "For more resources on python classes (we're relying on them all the time via sklearn!) see <a href = \"https://docs.python.org/3/tutorial/classes.html#a-first-look-at-classes\">this link.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design matrix shape (569, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texture error</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>mean radius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.565265</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.316862</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>0.660820</td>\n",
       "      <td>1.937015</td>\n",
       "      <td>0.907083</td>\n",
       "      <td>1.097064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.876244</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>-0.692926</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>0.260162</td>\n",
       "      <td>0.281190</td>\n",
       "      <td>-0.099444</td>\n",
       "      <td>1.829821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.780083</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.814974</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>1.424827</td>\n",
       "      <td>0.201391</td>\n",
       "      <td>0.293559</td>\n",
       "      <td>1.579888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.110409</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>2.744280</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.115007</td>\n",
       "      <td>4.935010</td>\n",
       "      <td>2.047511</td>\n",
       "      <td>-0.768909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   texture error  worst texture  worst symmetry  compactness error  worst radius  worst compactness  concave points error  worst fractal dimension  fractal dimension error  mean radius\n",
       "0      -0.565265      -1.359293        2.750622           1.316862      1.886690           2.616665              0.660820                 1.937015                 0.907083     1.097064\n",
       "1      -0.876244      -0.369203       -0.243890          -0.692926      1.805927          -0.430444              0.260162                 0.281190                -0.099444     1.829821\n",
       "2      -0.780083      -0.023974        1.152255           0.814974      1.511870           1.082932              1.424827                 0.201391                 0.293559     1.579888\n",
       "3      -0.110409       0.133984        6.046041           2.744280     -0.281464           3.893397              1.115007                 4.935010                 2.047511    -0.768909"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target classes    {1: 'Benign', 0: 'Malignant'}\n",
      "There are 357 Benign cases and 212 Malignant cases in the target\n",
      "Shape of Training Set : (448, 10)\n",
      "Shape of Testing Set : (121, 10)\n"
     ]
    }
   ],
   "source": [
    "from functions import tree_pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "get_tree_pd = tree_pd.get_tree_pd\n",
    "cancer_scaled, target = tree_pd.load_cancer_dataset(10, 4)\n",
    "\n",
    "################################### Train Test split\n",
    "np.random.seed(40)\n",
    "\n",
    "#test_proportion\n",
    "test_prop = 0.2\n",
    "msk = np.random.uniform(0, 1, len(cancer_scaled)) > test_prop\n",
    "\n",
    "#Split predictor and response columns\n",
    "X_train, y_train =  cancer_scaled[msk], target[msk]\n",
    "X_test , y_test  = cancer_scaled[~msk], target[~msk]\n",
    "\n",
    "print(\"Shape of Training Set :\", X_train.shape)\n",
    "print(\"Shape of Testing Set :\" , X_test.shape)\n",
    "\n",
    "################################### Train a bagging and random forest model\n",
    "\n",
    "depth = 13\n",
    "n_estimators = 100\n",
    "best_rf_model = RandomForestClassifier(max_depth=depth, random_state = 42, n_estimators= n_estimators)\n",
    "best_rf_model.fit(X_train, y_train.reshape(-1,))\n",
    "tree_rf_accuracy = best_rf_model.score(X_test, y_test.reshape(-1,))\n",
    "\n",
    "\n",
    "bagging_model = BaggingRegressor(DecisionTreeClassifier(max_depth=depth), \n",
    "                                 n_estimators = 100,\n",
    "                                 random_state = 42).fit(X_train, y_train.reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directions \n",
    "Run the cell below and look at the output. The .estimators_ attribute of a RandomForestClassifier class instance is a list of the individual DecisionTreeClassifier class instance estimators that make up the ensemble model. Calling .tree_ on the DecisionTreeClassifier will give you the individual tree estimator. \n",
    "1. Complete the function by extracting the impurity and feature attributes for each decision tree estimator at a specific decision node.\n",
    "2. Fix the creation of the dictionary at the bottom of the function and return a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tree object:\n",
      "\n",
      "class Tree(builtins.object)\n",
      " |  Array-based representation of a binary decision tree.\n",
      " |  \n",
      " |  The binary tree is represented as a number of parallel arrays. The i-th\n",
      " |  element of each array holds information about the node `i`. Node 0 is the\n",
      " |  tree's root. You can find a detailed description of all arrays in\n",
      " |  `_tree.pxd`. NOTE: Some of the arrays only apply to either leaves or split\n",
      " |  nodes, resp. In this case the values of nodes of the other type are\n",
      " |  arbitrary!\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  node_count : int\n",
      " |      The number of nodes (internal nodes + leaves) in the tree.\n",
      " |  \n",
      " |  capacity : int\n",
      " |      The current capacity (i.e., size) of the arrays, which is at least as\n",
      " |      great as `node_count`.\n",
      " |  \n",
      " |  max_depth : int\n",
      " |      The depth of the tree, i.e. the maximum depth of its leaves.\n",
      " |  \n",
      " |  children_left : array of int, shape [node_count]\n",
      " |      children_left[i] holds the node id of the left child of node i.\n",
      " |      For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
      " |      children_left[i] > i. This child handles the case where\n",
      " |      X[:, feature[i]] <= threshold[i].\n",
      " |  \n",
      " |  children_right : array of int, shape [node_count]\n",
      " |      children_right[i] holds the node id of the right child of node i.\n",
      " |      For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
      " |      children_right[i] > i. This child handles the case where\n",
      " |      X[:, feature[i]] > threshold[i].\n",
      " |  \n",
      " |  feature : array of int, shape [node_count]\n",
      " |      feature[i] holds the feature to split on, for the internal node i.\n",
      " |  \n",
      " |  threshold : array of double, shape [node_count]\n",
      " |      threshold[i] holds the threshold for the internal node i.\n",
      " |  \n",
      " |  value : array of double, shape [node_count, n_outputs, max_n_classes]\n",
      " |      Contains the constant prediction value of each node.\n",
      " |  \n",
      " |  impurity : array of double, shape [node_count]\n",
      " |      impurity[i] holds the impurity (i.e., the value of the splitting\n",
      " |      criterion) at node i.\n",
      " |  \n",
      " |  n_node_samples : array of int, shape [node_count]\n",
      " |      n_node_samples[i] holds the number of training samples reaching node i.\n",
      " |  \n",
      " |  weighted_n_node_samples : array of int, shape [node_count]\n",
      " |      weighted_n_node_samples[i] holds the weighted number of training samples\n",
      " |      reaching node i.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(...)\n",
      " |      Getstate re-implementation, for pickling.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Reduce re-implementation, for pickling.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      Setstate re-implementation, for unpickling.\n",
      " |  \n",
      " |  apply(...)\n",
      " |      Finds the terminal region (=leaf node) for each sample in X.\n",
      " |  \n",
      " |  compute_feature_importances(...)\n",
      " |      Computes the importance of each feature (aka variable).\n",
      " |  \n",
      " |  compute_partial_dependence(...)\n",
      " |      Partial dependence of the response on the ``target_feature`` set.\n",
      " |      \n",
      " |      For each sample in ``X`` a tree traversal is performed.\n",
      " |      Each traversal starts from the root with weight 1.0.\n",
      " |      \n",
      " |      At each non-leaf node that splits on a target feature, either\n",
      " |      the left child or the right child is visited based on the feature\n",
      " |      value of the current sample, and the weight is not modified.\n",
      " |      At each non-leaf node that splits on a complementary feature,\n",
      " |      both children are visited and the weight is multiplied by the fraction\n",
      " |      of training samples which went to each child.\n",
      " |      \n",
      " |      At each leaf, the value of the node is multiplied by the current\n",
      " |      weight (weights sum to 1 for all visited terminal nodes).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : view on 2d ndarray, shape (n_samples, n_target_features)\n",
      " |          The grid points on which the partial dependence should be\n",
      " |          evaluated.\n",
      " |      target_features : view on 1d ndarray, shape (n_target_features)\n",
      " |          The set of target features for which the partial dependence\n",
      " |          should be evaluated.\n",
      " |      out : view on 1d ndarray, shape (n_samples)\n",
      " |          The value of the partial dependence function on each grid\n",
      " |          point.\n",
      " |  \n",
      " |  decision_path(...)\n",
      " |      Finds the decision path (=node) for each sample in X.\n",
      " |  \n",
      " |  predict(...)\n",
      " |      Predict target for X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  capacity\n",
      " |  \n",
      " |  children_left\n",
      " |  \n",
      " |  children_right\n",
      " |  \n",
      " |  feature\n",
      " |  \n",
      " |  impurity\n",
      " |  \n",
      " |  max_depth\n",
      " |  \n",
      " |  max_n_classes\n",
      " |  \n",
      " |  n_classes\n",
      " |  \n",
      " |  n_features\n",
      " |  \n",
      " |  n_leaves\n",
      " |  \n",
      " |  n_node_samples\n",
      " |  \n",
      " |  n_outputs\n",
      " |  \n",
      " |  node_count\n",
      " |  \n",
      " |  threshold\n",
      " |  \n",
      " |  value\n",
      " |  \n",
      " |  weighted_n_node_samples\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(best_rf_model.estimators_[0].tree_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exercises/bo1.py\"\n",
    "def get_impurity_pd(model, n = 0):\n",
    "    \"\"\"\n",
    "    This function returns a pandas dataframe with all of the nth nodes feature impurities.\n",
    "    \"\"\"\n",
    "    rf_estimators = model.estimators_.copy()\n",
    "    features = np.array(X_train.columns)\n",
    "    \n",
    "    node_impurities, node_features = [], []\n",
    "\n",
    "    for i, estimator in enumerate(rf_estimators):\n",
    "        estimator_impurity = #TODO 0 \n",
    "        estimator_feature  = #TODO 1\n",
    "        \n",
    "        node_impurities.append(estimator_impurity)\n",
    "        node_features.append(estimator_feature)\n",
    "    node_impurity_dict = {\"feature\": #TODO \n",
    "                        \"impurity\": #TODO\n",
    "    df = #TODO\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/impurity.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_node = 0\n",
    "rf_df = get_impurity_pd(best_rf_model, tree_node)\n",
    "bagging_df = get_impurity_pd(bagging_model, tree_node)\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(1,2, figsize = (20, 5))\n",
    "ax.ravel()\n",
    "\n",
    "sns.swarmplot(x = \"feature\", y = \"impurity\", data = rf_df, ax = ax[0])\n",
    "#sns.swarmplot(x = \"feature\", y = \"impurities\", data = rf_df, ax = ax[0])\n",
    "ax[0].tick_params(labelrotation=45)\n",
    "ax[0].set_title(\"Random Forest: Node 0 impurities after split\")\n",
    "\n",
    "sns.swarmplot(x = \"feature\", y = \"impurity\", data = bagging_df, ax = ax[1])\n",
    "ax[1].set_title(\"Bagging: Node 0 impurities after split\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "\n",
    "## The limitations of random forest\n",
    "\n",
    "#### When can Random Forest overfit? \n",
    "- Increasing the number of trees in RF generally doesn't increase the risk of overfitting, BUT if the number of trees in the ensemble is too large then the trees in the ensemble may become correlated, and therefore increase the variance.\n",
    "\n",
    "#### When can Random Forest fail? \n",
    "\n",
    "- **When we have a lot of predictors that are completely independent of the response and one overwhelmingly influential predictor**.\n",
    "\n",
    "#### Why aren't random forests and bagging interpretable?  How about a very deep decision tree?\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and random forest vs. Boosting\n",
    "\n",
    "- **Bagging and Random Forest:**\n",
    "  - complex and deep trees **overfit**\n",
    "  - thus **let's perform variance reduction on complex trees!**\n",
    "- **Boosting:** \n",
    "  - simple and shallow trees **underfit** \n",
    "  - thus **let's perform bias reduction of simple trees!**\n",
    "  - make the simple trees more expressive!\n",
    "  \n",
    "**Boosting** attempts to improve the predictive flexibility of simple models.\n",
    " - It trains a **large number of ‚Äúweak‚Äù learners in sequence**.\n",
    " - A weak learner is a constrained model (limit the max depth of each decision tree).\n",
    " - Each one in the sequence focuses on **learning from the mistakes** of the one before it.\n",
    " - By more heavily weighting in the mistakes in the next tree, our next tree will learn from the mistakes.\n",
    " - A combining all the weak learners into a single strong learner = **a boosted tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting1.png?\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "----------\n",
    "\n",
    "### Illustrative example (from [source](https://towardsdatascience.com/underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6fe4a8a49dbf))\n",
    "\n",
    "<img src=\"data/boosting.png\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built multiple trees consecutively: Tree 1 -> Tree 2 -> Tree 3 - > ....\n",
    "\n",
    "**The size of the plus or minus signs indicates the weights of a data points for every Tree**. How do we determine these weights?\n",
    "\n",
    "For each consecutive tree and iteration we do the following:\n",
    " - The **wrongly classified data points (\"mistakes\" = red circles)** are identified and **more heavily weighted in the next tree (green arrow)**. \n",
    " - Thus the size of the plus or minus changes in the next tree\n",
    " - This change in weights will influence and change the next simple decision tree\n",
    " - The **correct predictions are** identified and **less heavily weighted in the next tree**.\n",
    "\n",
    "We iterate this process for a certain number of times, stop and construct our final model: \n",
    "- The ensemble (**\"Final: Combination\"**) is a linear combination of the simple trees, and is more expressive!\n",
    "- The ensemble (**\"Final: Combination\"**) has indeed not just one simple decision boundary line, and fits the data better.\n",
    " \n",
    " \n",
    "<img src=\"data/boosting_2.png?\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ada Boost?\n",
    "\n",
    "- Ada Boost = Adaptive Boosting.\n",
    "- AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers\n",
    "\n",
    "<img src=\"data/AdaBoost1.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "<img src=\"data/AdaBoost2.png\" alt=\"tree_adj\" width=\"70%\"/>\n",
    "\n",
    "For an individual training point the loss can be defined as:\n",
    "$$\\text{ExpLoss_i} = \\begin{cases}\n",
    "      e^{\\hat{y}}, & \\text{if}\\ y=-1 \\\\\n",
    "      e^{-\\hat{y}}, & \\text{} y=1\n",
    "    \\end{cases}\n",
    "$$\n",
    "<img src=\"data/AdaBoost3.png\" alt=\"tree_adj\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example (from slides)\n",
    "------\n",
    "**Step1. Start with equal distribition initially**\n",
    "<img src=\"data/ADA2.png\" alt=\"tree_adj\" width=\"40%\">\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA3.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "------\n",
    "**Step3. Update the weights**\n",
    "<img src=\"data/ADA4.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step4. Update the classifier:** First time trivial (we have no model yet.)\n",
    "\n",
    "------\n",
    "**Step2. Fit a simple classifier**\n",
    "<img src=\"data/ADA5.png\" alt=\"tree_adj\" width=\"40%\"/>\n",
    "\n",
    "**Step3. Update the weights:** not shown.\n",
    "\n",
    "------\n",
    "**Step4. Update the classifier:**\n",
    "<img src=\"data/ADA6.png\" alt=\"tree_adj\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the Adaboost method to visualize Bias-Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-07bcd6f06ff0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Fit an Adaboost Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Fit an Adaboost Model\n",
    "\n",
    "x_train, y_train = data_train.drop(['Spam'], axis=1), data_train['Spam']\n",
    "x_test , y_test  = data_test.drop(['Spam'] , axis=1), data_test['Spam']\n",
    "\n",
    "#Training\n",
    "model = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3), \n",
    "                           n_estimators=200, \n",
    "                           learning_rate=0.05)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "acc_boosting_training = accuracy_score(y_train, y_pred_train)*100\n",
    "acc_boosting_test = accuracy_score(y_test, y_pred_test)*100\n",
    "\n",
    "print(\"Ada Boost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_boosting_training))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the test and training accuracy evolve with every iteration (tree)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Iteration based score\n",
    "train_scores = list(model.staged_score(x_train,y_train))\n",
    "test_scores = list(model.staged_score(x_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(train_scores,label='train')\n",
    "plt.plot(test_scores,label='test')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Variation of Accuracy with Iterations - ADA Boost\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost seems to be performing better than Simple Decision Trees and has a similar Test Set Accuracy performance compared to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random tip:** If a \"for\"-loop takes som time and you want to know the progress while running the loop, use: **tqdm()** ([link](https://github.com/tqdm/tqdm)). No need for 1000's of ```print(i)``` outputs.\n",
    "\n",
    "\n",
    "Usage: ```for i in tqdm( range(start,finish) ):```\n",
    "\n",
    " - tqdm means *\"progress\"* in Arabic (taqadum, ÿ™ŸÇÿØŸëŸÖ) and \n",
    " - tqdm is an abbreviation for *\"I love you so much\"* in Spanish (te quiero demasiado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we change the depth of our AdaBoost trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\david\\anaconda3\\envs\\cs109a\\lib\\site-packages (4.48.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py @ file:///D:/bld/absl-py_1598382479526/work\n",
      "argon2-cffi @ file:///D:/bld/argon2-cffi_1596630110014/work\n",
      "arviz @ file:///home/conda/feedstock_root/build_artifacts/arviz_1592923948739/work\n",
      "astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work\n",
      "astunparse==1.6.3\n",
      "atomicwrites @ file:///home/conda/feedstock_root/build_artifacts/atomicwrites_1588182545583/work\n",
      "attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1599308529326/work\n",
      "autograd==1.3\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache==1.6.1\n",
      "beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1597679909012/work\n",
      "bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1588608214987/work\n",
      "blinker==1.4\n",
      "blis==0.4.1\n",
      "bokeh @ file:///D:/bld/bokeh_1599001193669/work\n",
      "boto==2.49.0\n",
      "boto3 @ file:///home/conda/feedstock_root/build_artifacts/boto3_1599255782083/work\n",
      "botocore @ file:///home/conda/feedstock_root/build_artifacts/botocore_1599252014234/work\n",
      "brotlipy==0.7.0\n",
      "bz2file==0.98\n",
      "cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1593420445823/work\n",
      "catalogue==1.0.0\n",
      "certifi==2020.6.20\n",
      "cffi @ file:///D:/bld/cffi_1595805721274/work\n",
      "cftime @ file:///D:/bld/cftime_1595215844964/work\n",
      "chardet==3.0.4\n",
      "click==7.1.2\n",
      "cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1598400192773/work\n",
      "colorama==0.4.3\n",
      "cryptography @ file:///D:/bld/cryptography_1598621309630/work\n",
      "cycler==0.10.0\n",
      "cymem @ file:///D:/bld/cymem_1591801268646/work\n",
      "cytoolz==0.10.1\n",
      "dash @ file:///home/conda/feedstock_root/build_artifacts/dash_1596870105808/work\n",
      "dash-core-components @ file:///home/conda/feedstock_root/build_artifacts/dash-core-components_1596203643601/work\n",
      "dash-html-components==1.0.3\n",
      "dash-renderer @ file:///home/conda/feedstock_root/build_artifacts/dash-renderer_1596203647496/work\n",
      "dash-table @ file:///home/conda/feedstock_root/build_artifacts/dash-table_1596867885662/work\n",
      "dask @ file:///home/conda/feedstock_root/build_artifacts/dask-core_1598661378447/work\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "docutils==0.15.2\n",
      "entrypoints==0.3\n",
      "fastprogress==1.0.0\n",
      "Flask==1.1.2\n",
      "Flask-Compress==1.5.0\n",
      "future==0.18.2\n",
      "gap-stat==2.0.1\n",
      "gast==0.3.3\n",
      "gensim @ file:///D:/bld/gensim_1589441301799/work\n",
      "google-api-core @ file:///D:/bld/google-api-core-split_1599169078462/work\n",
      "google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1599161935305/work\n",
      "google-auth-oauthlib==0.4.1\n",
      "google-cloud-core @ file:///home/conda/feedstock_root/build_artifacts/google-cloud-core_1596721852962/work\n",
      "google-cloud-storage @ file:///home/conda/feedstock_root/build_artifacts/google-cloud-storage_1598557481535/work\n",
      "google-crc32c @ file:///D:/bld/google-crc32c_1597070112100/work\n",
      "google-pasta==0.2.0\n",
      "google-resumable-media @ file:///home/conda/feedstock_root/build_artifacts/google-resumable-media_1598452053521/work\n",
      "googleapis-common-protos==1.51.0\n",
      "graphviz @ file:///home/conda/feedstock_root/build_artifacts/python-graphviz_1602067273230/work\n",
      "grpcio @ file:///D:/bld/grpcio_1596715850903/work\n",
      "h5py @ file:///D:/bld/h5py_1595110299148/work\n",
      "holoviews @ file:///home/conda/feedstock_root/build_artifacts/holoviews_1592928184246/work\n",
      "idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1593328102638/work\n",
      "imagecodecs @ file:///D:/bld/imagecodecs_1593295625443/work\n",
      "imageio @ file:///home/conda/feedstock_root/build_artifacts/imageio_1594044661732/work\n",
      "importlib-metadata @ file:///D:/bld/importlib-metadata_1593211612489/work\n",
      "iniconfig @ file:///home/conda/feedstock_root/build_artifacts/iniconfig_1596221474428/work\n",
      "ipykernel @ file:///D:/bld/ipykernel_1595447157738/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipytest @ file:///home/conda/feedstock_root/build_artifacts/ipytest_1594663989576/work\n",
      "ipython @ file:///D:/bld/ipython_1598750239682/work\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "itsdangerous==1.1.0\n",
      "jedi @ file:///D:/bld/jedi_1595018503144/work\n",
      "Jinja2==2.11.2\n",
      "jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1589369830981/work\n",
      "joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1593624380152/work\n",
      "json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1591810480056/work\n",
      "jsonschema==3.2.0\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1598486169312/work\n",
      "jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1598728807792/work\n",
      "jupyter-contrib-core==0.3.3\n",
      "jupyter-core==4.6.3\n",
      "jupyter-nbextensions-configurator @ file:///D:/bld/jupyter_nbextensions_configurator_1594127290135/work\n",
      "jupyterlab==2.2.6\n",
      "jupyterlab-server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server_1593951277307/work\n",
      "jupyterthemes==0.20.0\n",
      "Keras==2.4.3\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.2.0\n",
      "lesscpy==0.13.0\n",
      "Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1589366472132/work\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib @ file:///D:/bld/matplotlib-base_1597952542171/work\n",
      "mistune==0.8.4\n",
      "mkl-service==2.3.0\n",
      "more-itertools @ file:///home/conda/feedstock_root/build_artifacts/more-itertools_1598643641143/work\n",
      "murmurhash==1.0.0\n",
      "nbconvert==5.6.1\n",
      "nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1594060262917/work\n",
      "netCDF4 @ file:///D:/bld/netcdf4_1598211768602/work\n",
      "networkx @ file:///home/conda/feedstock_root/build_artifacts/networkx_1598210780226/work\n",
      "nltk==3.4.4\n",
      "notebook @ file:///D:/bld/notebook_1597285459320/work\n",
      "numpy==1.18.5\n",
      "oauthlib==3.0.1\n",
      "olefile==0.46\n",
      "opt-einsum==3.3.0\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1589925210001/work\n",
      "pandas @ file:///D:/bld/pandas_1598294826108/work\n",
      "pandocfilters==1.4.2\n",
      "panel @ file:///home/conda/feedstock_root/build_artifacts/panel_1592920888719/work\n",
      "param==1.9.3\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1595548966091/work\n",
      "patsy==0.5.1\n",
      "pickleshare==0.7.5\n",
      "Pillow @ file:///D:/bld/pillow_1594213048891/work\n",
      "plac==0.9.6\n",
      "plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1594904902831/work\n",
      "pluggy @ file:///D:/bld/pluggy_1592827687923/work\n",
      "ply==3.11\n",
      "preshed @ file:///D:/bld/preshed_1591802333328/work\n",
      "progressbar2==3.52.1\n",
      "prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1590412252446/work\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1598885455507/work\n",
      "protobuf==3.13.0\n",
      "py @ file:///home/conda/feedstock_root/build_artifacts/py_1593088446458/work\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.7\n",
      "pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1593275161868/work\n",
      "pyct==0.4.6\n",
      "Pygments==2.6.1\n",
      "PyJWT==1.7.1\n",
      "pymc3==3.9.3\n",
      "pyOpenSSL==19.1.0\n",
      "pyparsing==2.4.7\n",
      "PyQt5==5.12.3\n",
      "PyQt5-sip==4.19.18\n",
      "PyQtWebEngine==5.12.1\n",
      "pyreadline==2.1\n",
      "pyrsistent==0.16.0\n",
      "PySocks==1.7.1\n",
      "pytest==6.0.1\n",
      "python-dateutil==2.8.1\n",
      "python-utils==2.4.0\n",
      "pytz==2020.1\n",
      "pyviz-comms @ file:///home/conda/feedstock_root/build_artifacts/pyviz_comms_1594121601757/work\n",
      "PyWavelets==1.1.1\n",
      "pywin32==227\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole_1599147533948/work\n",
      "QtPy==1.9.0\n",
      "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1592425495151/work\n",
      "requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1595492159598/work\n",
      "retrying==1.3.3\n",
      "rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1591996208734/work\n",
      "s3transfer==0.3.3\n",
      "scikit-image==0.17.2\n",
      "scikit-learn @ file:///D:/bld/scikit-learn_1596546337481/work\n",
      "scipy==1.4.1\n",
      "seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-base_1591878760859/work\n",
      "Send2Trash==1.5.0\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1590081179328/work\n",
      "smart-open @ file:///home/conda/feedstock_root/build_artifacts/smart_open_1598572939086/work\n",
      "soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1597680516047/work\n",
      "spacy @ file:///D:/bld/spacy_1594659295035/work\n",
      "srsly @ file:///D:/bld/srsly_1589222538360/work\n",
      "statsmodels @ file:///D:/bld/statsmodels_1598551192913/work\n",
      "tensorboard==2.1.1\n",
      "tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1592816951245/work/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl\n",
      "tensorflow==2.3.1\n",
      "tensorflow-estimator==2.3.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.3\n",
      "testpath==0.4.4\n",
      "Theano==1.0.5\n",
      "thinc @ file:///D:/bld/thinc_1590400575886/work\n",
      "threadpoolctl @ file:///tmp/tmp79xdzxkt/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "tifffile @ file:///home/conda/feedstock_root/build_artifacts/tifffile_1599208671228/work\n",
      "toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1589469402899/work\n",
      "toolz==0.10.0\n",
      "tornado==6.0.4\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1596476591553/work\n",
      "traitlets==4.3.3\n",
      "typing-extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1588470653596/work\n",
      "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1595434816409/work\n",
      "wasabi @ file:///home/conda/feedstock_root/build_artifacts/wasabi_1594929739383/work\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1595859607677/work\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.16.1\n",
      "widgetsnbextension @ file:///D:/bld/widgetsnbextension_1594164533747/work\n",
      "win-inet-pton==1.1.0\n",
      "wincertstore==0.2\n",
      "wrapt==1.12.1\n",
      "xarray @ file:///home/conda/feedstock_root/build_artifacts/xarray_1594532077462/work\n",
      "xlrd==1.2.0\n",
      "zipp==3.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a7729fe7ce30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Find Optimal Depth of trees for Boosting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mscore_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     model = AdaBoostClassifier(\n\u001b[0;32m     10\u001b[0m         \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Start Timer\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train, score_test, depth_start, depth_end = {}, {}, 2, 30\n",
    "for i in tqdm(range(depth_start, depth_end, 2)):\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=i),\n",
    "        n_estimators=200, learning_rate=0.05)\n",
    "    model.fit(x_train, y_train)\n",
    "    score_train[i] = accuracy_score(y_train, model.predict(x_train))\n",
    "    score_test[i] = accuracy_score(y_test, model.predict(x_test))\n",
    "    \n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_adaboost = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train.items())\n",
    "lists2 = sorted(score_test.items())\n",
    "x1, y1 = zip(*lists1) \n",
    "x2, y2 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - ADA Boost Classifier')\n",
    "plt.plot(x1, y1, 'b-', label='Train')\n",
    "plt.plot(x2, y2, 'g-', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost complexity depends on both the number of estimators and the base estimator. \n",
    "- In the beginning as our model complexity increases (depth 2-3), we first observe a small increase in accuracy.\n",
    "- But as we go further to the right of the graph (**deeper trees**), our model **will overfit the data.**\n",
    "- **REMINDER and validation: Boosting relies on simple trees!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout Room 2: Explore how changing the learning rate changes the training and testing accuracy. Use the Te Quero Demasiado (TQDM) wrap around your range as above. (Hint you will probably want to explore a range from $e^{-6}$ to $e^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a0b978b5f34ae086e82faaa57f48bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm as tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "score_train, score_test, depth_start, depth_end = {}, {}, 2, 30\n",
    "for i in trange(-6, -1, 1):\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=4),\n",
    "        n_estimators=200, learning_rate=np.exp(i))\n",
    "    model.fit(x_train, y_train)\n",
    "    score_train[i] = accuracy_score(y_train, model.predict(x_train))\n",
    "    score_test[i] = accuracy_score(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists1 = sorted(score_train.items())\n",
    "lists2 = sorted(score_test.items())\n",
    "x1, y1 = zip(*lists1) \n",
    "x2, y2 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"lr\")\n",
    "plt.title('Variation of Accuracy with Depth - ADA Boost Classifier')\n",
    "plt.plot(np.exp(x1), y1, 'b-', label='Train')\n",
    "plt.plot(np.exp(x2), y2, 'g-', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/bo2.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this exercise useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food for Thought :**\n",
    "- Are **boosted models independent of one another?** Do they need to wait for the previous model's residuals?\n",
    "- Are **bagging or random forest models independent of each other**, can they be trained in a parallel fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Theory:* What is Gradient Boosting and XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "To improve its predictions, **gradient boosting looks at the difference between its current approximation, and the known correct target vector, which is called the residual**.\n",
    "\n",
    "The mathematics:\n",
    "\n",
    "- It may be assumed that there is some imperfect model $F_{m}$ \n",
    "- The gradient boosting algorithm improves on $F_{m}$ constructing a new model that adds an estimator $h$ to provide a better model: \n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)$$\n",
    "\n",
    "- To find $h$, the gradient boosting solution starts with the observation that a perfect **h** would imply\n",
    "\n",
    "$$F_{m+1}(x)=F_{m}(x)+h(x)=y$$\n",
    "\n",
    "- or, equivalently solving for h,\n",
    "\n",
    "$$h(x)=y-F_{m}(x)$$\n",
    "\n",
    "- Therefore, gradient boosting will fit h to the residual $y-F_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/gradient_boosting2.png\" alt=\"tree_adj\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "\n",
    "### XGBoost: [\"Long May She Reign!\"](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/kaggle.png\" alt=\"tree_adj\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "### What is XGBoost and why is it so good!?\n",
    " - Based on Gradient Boosting\n",
    " - XGBoost = **eXtreme Gradient Boosting**; refers to the engineering goal to push the limit of computations resources for boosted tree algorithm\n",
    " \n",
    "**Accuracy:**\n",
    " - XGBoost however uses a **more regularized model formalizaiton to control overfitting** (=better performance) by both L1 and L2 regularization.\n",
    " - Tree Pruning methods: more shallow tree will also prevent overfitting\n",
    " - Improved convergence techniques (like early stopping when no improvement is made for X number of iterations)\n",
    " - Built-in Cross-Validaiton\n",
    " \n",
    "**Computing Speed:**\n",
    " - Special Vector and matrix type data structures for faster results.\n",
    " - Parallelized tree building: using all of your CPU cores during training.\n",
    " - Distributed Computing: for training very large models using a cluster of machines.\n",
    " - Cache Optimization of data structures and algorithm: to make best use of hardware.\n",
    "\n",
    "**XGBoost is building boosted trees in parallel? What? How?**\n",
    "- No: Xgboost doesn't run multiple trees in parallel, you need predictions after each tree to update gradients.\n",
    "- Rather it does the parallelization WITHIN a single tree my using openMP to create branches independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use XGBoost: Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.2.1-py3-none-win_amd64.whl (86.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\david\\anaconda3\\envs\\cs109a\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\david\\appdata\\roaming\\python\\python37\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.1\n"
     ]
    }
   ],
   "source": [
    "# Let's install XGBoost\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.07756\n",
      "Will train until train-merror hasn't improved in 20 rounds.\n",
      "[20]\ttrain-merror:0.02698\n",
      "[40]\ttrain-merror:0.01563\n",
      "[60]\ttrain-merror:0.00736\n",
      "[80]\ttrain-merror:0.00276\n",
      "[100]\ttrain-merror:0.00153\n",
      "[120]\ttrain-merror:0.00092\n",
      "Stopping. Best iteration:\n",
      "[101]\ttrain-merror:0.00092\n",
      "\n",
      "XGBoost:\tAccuracy, Training Set \t: 99.91%\n",
      "XGBoost:\tAccuracy, Testing Set \t: 94.92%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create the training and test data\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# Parameters\n",
    "param = {\n",
    "    'max_depth': 5,  # the maximum depth of each tree\n",
    "    'eta': 0.3,               # the training step for each iteration\n",
    "    'silent': 1,              # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}           # the number of classes that exist in this datset\n",
    "\n",
    "# Number of training iterations\n",
    "num_round = 200  \n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Train XGBoost\n",
    "bst = xgb.train(param, \n",
    "                dtrain, \n",
    "                num_round, \n",
    "                evals= [(dtrain, 'train')], \n",
    "                early_stopping_rounds=20, # early stopping\n",
    "                verbose_eval=20)\n",
    "\n",
    "\n",
    "# Make prediction training set\n",
    "preds_train = bst.predict(dtrain)\n",
    "best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "\n",
    "# Make prediction test set\n",
    "preds_test = bst.predict(dtest)\n",
    "best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "# Performance Evaluation \n",
    "acc_XGBoost_training = accuracy_score(y_train, best_preds_train)*100\n",
    "acc_XGBoost_test = accuracy_score(y_test, best_preds_test)*100\n",
    "\n",
    "# Stop Timer\n",
    "end = time.time()\n",
    "elapsed_xgboost = end - start\n",
    "\n",
    "print(\"XGBoost:\\tAccuracy, Training Set \\t: {:0.2f}%\".format(acc_XGBoost_training))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the accuracy performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_boosting_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-24683ec254b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_boosting_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_XGBoost_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_boosting_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the computing performance: AdaBoost versus XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elapsed_adaboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-fcfeaf3b2868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AdaBoost elapsed time: \\t{:0.2f}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_adaboost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XGBoost elapsed time: \\t{:0.2f}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_xgboost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'elapsed_adaboost' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"AdaBoost elapsed time: \\t{:0.2f}s\".format(elapsed_adaboost))\n",
    "print(\"XGBoost elapsed time: \\t{:0.2f}s\".format(elapsed_xgboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change the depth of our XGBoost trees and compare to Ada Boost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgboost(best_depth):\n",
    "    param = {\n",
    "    'max_depth': best_depth,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'verbosity': 0,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}  # the number of classes that exist in this datset\n",
    "\n",
    "    # the number of training iterations\n",
    "    num_round = 200  \n",
    "\n",
    "    bst = xgb.train(param, \n",
    "                    dtrain, \n",
    "                    num_round, \n",
    "                    evals= [(dtrain, 'train')], \n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    preds_train = bst.predict(dtrain)\n",
    "    best_preds_train = np.asarray([np.argmax(line) for line in preds_train])\n",
    "    preds_test = bst.predict(dtest)\n",
    "    best_preds_test = np.asarray([np.argmax(line) for line in preds_test])\n",
    "\n",
    "    #Performance Evaluation\n",
    "    XGBoost_training = accuracy_score(y_train, best_preds_train)\n",
    "    XGBoost_test = accuracy_score(y_test, best_preds_test)\n",
    "    \n",
    "    return XGBoost_training, XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb145f5ed2146a38b88a08d97fdf264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Find Optimal Depth of trees for Boosting\n",
    "score_train_xgb, score_test_xgb = {}, {}\n",
    "depth_start, depth_end = 2, 30\n",
    "for i in trange(depth_start, depth_end, 2):\n",
    "    XGBoost_training, XGBoost_test = model_xgboost(i)\n",
    "    score_train_xgb[i] = XGBoost_training\n",
    "    score_test_xgb[i] = XGBoost_test"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> eea2ee9075202395cbb2d30b9a78c9d86a699ec6
   "source": [
    "#Plot\n",
    "lists1 = sorted(score_train_xgb.items())\n",
    "lists2 = sorted(score_test_xgb.items())\n",
    "x3, y3 = zip(*lists1) \n",
    "x4, y4 = zip(*lists2) \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.title('Variation of Accuracy with Depth - Adaboost & XGBoost Classifier')\n",
    "plt.plot(x1, y1, label='Train Accuracy Ada Boost')\n",
    "plt.plot(x2, y2, label='Test Accuracy Ada Boost')\n",
    "plt.plot(x3, y3, label='Train Accuracy XGBoost')\n",
    "plt.plot(x4, y4, label='Test Accuracy XGBoost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting**: \n",
    "- No real optimal depth of the simple tree for XGBoost, probably a lot of regularization, pruning, or early stopping when using a deep tree at the start.\n",
    "- XGBoost does not seem to overfit when the depth of the tree increases, as opposed to Ada Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the accuracy performances:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_trees_testing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7ea3459011e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_trees_testing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0macc_bagging_testing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_random_forest_testing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_boosting_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_XGBoost_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_trees_testing' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Decision Trees:\\tAccuracy, Testing Set \\t: {:.2%}\".format(acc_trees_testing))\n",
    "print(\"Bagging: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format( acc_bagging_testing))\n",
    "print(\"Random Forest: \\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_random_forest_testing))\n",
    "print(\"Ada Boost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_boosting_test))\n",
    "print(\"XGBoost:\\tAccuracy, Testing Set \\t: {:0.2f}%\".format(acc_XGBoost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------\n",
    "\n",
    "**Overview of all the tree algorithms:** [Source](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)\n",
    "\n",
    "<img src=\"data/trees.png\" alt=\"tree_adj\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "\n",
    "## End of Section\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Example to better understand Bias vs Variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion underlying what we've been learning in lectures and sections so far is the trade-off between overfitting and underfitting. If you remember back to Homework 3, we had a model that seemed to represent our data accurately. However, we saw that as we made it more and more accurate on the training set, it did not generalize well to unobserved data.\n",
    "\n",
    "As a different example, in face recognition algorithms, such as that on the iPhone X, a too-accurate model would be unable to identity someone who styled their hair differently that day. The reason is that our model may learn irrelevant features in the training data. On the contrary, an insufficiently trained model would not generalize well either. For example, it was recently reported that a face mask could sufficiently fool the iPhone X.\n",
    "\n",
    "A widely used solution in statistics to reduce overfitting consists of adding structure to the model, with something like regularization. This method favors simpler models during training.\n",
    "\n",
    "The bias-variance dilemma is closely related. \n",
    "- The **bias** of a model quantifies how precise a model is across training sets. \n",
    "- The **variance** quantifies how sensitive the model is to small changes in the training set. \n",
    "- A **robust** model is not overly sensitive to small changes. \n",
    "- **The dilemma involves minimizing both bias and variance**; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust.\n",
    "\n",
    "**How to reduce bias:**\n",
    " - **Use more complex models, more features, less regularization,** ...\n",
    " - **Boosting:** attempts to improve the predictive flexibility of simple models. Boosting uses simple base models and tries to ‚Äúboost‚Äù their aggregate complexity.\n",
    " \n",
    "**How to reduce variance:**\n",
    " - **Early Stopping:** Its rules provide us with guidance as to how many iterations can be run before the learner begins to over-fit.\n",
    " - **Pruning:** Pruning is extensively used while building related models. It simply removes the nodes which add little predictive power for the problem in hand.\n",
    " - **Regularization:** It introduces a cost term for bringing in more features with the objective function. Hence it tries to push the coefficients for many variables to zero and hence reduce cost term.\n",
    " - **Train with more data:** It won‚Äôt work every time, but training with more data can help algorithms detect the signal better.\n",
    " - **Ensembling:** Ensembles are machine learning methods for combining predictions from multiple separate models. For example:\n",
    "   - **Bagging** attempts to reduce the chance of overfitting complex models: Bagging uses complex base models and tries to ‚Äúsmooth out‚Äù their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "#### Interesting Piazza post: why randomness in simple decision tree?\n",
    " \n",
    " ```\"Hi there. I notice that there is a parameter called \"random_state\" in decision tree function and I wonder why we need randomness in simple decision tree. If we add randomness in such case, isn't it the same as random forest?\"```\n",
    " \n",
    "  - The problem of learning an optimal decision tree is known to be **NP-complete** under several aspects of optimality and even for simple concepts. \n",
    "  - Consequently, practical decision-tree learning algorithms are based on **heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node**. \n",
    "  - Such algorithms **cannot guarantee to return the globally optimal decision tree**. \n",
    "  - This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement (Bagging).\n",
    "  \n",
    "For example: **What is the defaulth DecisionTreeClassifier behaviour when there are 2 or more best features for a certain split (a tie among \"splitters\")?** (after a deep dive and internet search [link](https://github.com/scikit-learn/scikit-learn/issues/12259 ) ):\n",
    "\n",
    "  - The current default behaviour when splitter=\"best\" is to shuffle the features at each step and take the best feature to split. \n",
    "  - In case there is a tie, we take a random one."
   ]
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.8"
=======
   "version": "3.7.9"
>>>>>>> eea2ee9075202395cbb2d30b9a78c9d86a699ec6
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
