{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Homework 6  AC 209 : PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise'> <b> Question 1 [10 pts] </b> \n",
    "\n",
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n√óp}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "**1.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    "\n",
    "**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "**1.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**1.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    " \n",
    " $$ \\parallel A \\parallel ^2 = A^T A $$\n",
    " \n",
    " Then, express the reconstruction error as a sum of matrix products.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "$$  \\parallel XQ- XQ_m \\parallel ^2 =  \\parallel{XQ}\\parallel ^2 + \\parallel{XQ_m}\\parallel ^2  - 2 (XQ)^T(XQ_m) = \\parallel{XQ}\\parallel ^2 + \\parallel{XQ_m}\\parallel ^2  - 2 Q^TX^TXQ_m$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "$$  \\parallel XQ- XQ_m \\parallel ^2 = \\parallel{XQ}\\parallel ^2 + \\parallel{XQ_m}\\parallel ^2  - 2 Q^TX^TXQ_m = \\parallel{XQ}\\parallel ^2 + \\parallel{XQ_m}\\parallel ^2  - 2 Q^TQ \\Lambda Q ^TQ_m  = \\parallel{XQ}\\parallel ^2 + \\parallel{XQ_m}\\parallel ^2  - 2\\Lambda Q ^TQ_m$$  \n",
    "\n",
    "We also have :\n",
    "$$\n",
    " \\parallel{XQ}\\parallel ^2 = Q^TX^TXQ = Q^TQ \\Lambda Q ^TQ = \\Lambda\n",
    "$$\n",
    "$$\n",
    " \\parallel{XQ_m}\\parallel ^2 = Q_m^TX^TXQ_m = Q_m^TQ \\Lambda Q ^TQ_m \n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 = \\Lambda + Q_m^TQ \\Lambda Q ^TQ_m  - 2\\Lambda Q ^TQ_m$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    " \n",
    " $$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    " \n",
    " Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "$Q^T_m Q \\in \\Re^{p \\times p} $ and $\\quad \\forall (i <m ,j < p), \\quad [Q^T_m Q]_{ij} = <q_i, q_j>$ and all the other coefficients are 0 (the last $p-m$ lines are 0).\n",
    "\n",
    "$Q^TQ_m$ is the transpose of $Q^T_m Q$, therefore the elements of this matrix are the $<q_j, q_i>$ and all the last $p-m$ columns are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.4",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "From all the previous questions, we can express the reconstruction error as $Q^T\\Lambda_mQ$ where $\\Lambda_m = diag(0, ..., \\lambda_{m+1}, .., \\lambda_p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.5",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    " above result, namely\n",
    " $$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    " Use your result from 5.4 and this new definition to find a simple expression\n",
    " for the reconstruction error in terms of the eigenvalues.\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Since the trace is independent of the basis on which we express our matrix, we can express \n",
    "$$\n",
    " \\parallel XQ- XQ_m \\parallel ^2 = \\sum_{\\lambda \\in Sp(X^TX), i>m}^p\\lambda_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.6",
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>  \n",
    " \n",
    "**1.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    " \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We can see that our result shows that the reconstruction error depends **only** on the subspace dimension, which makes sense in terms of intuition. We can see that we recover the entire information for ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
